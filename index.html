<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Xorbas, a modified Hadoop HDFS with new storage codes</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">Xorbas Hadoop</h1>
        <p class="header">The code for Xorbas can be found below.</p>

        <ul>
          <li class="download"><a class="buttons" href="https://github.com/madiator/HadoopUSC/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/madiator/HadoopUSC/tarball/master">Download TAR</a></li>
          <li><a class="buttons github" href="https://github.com/madiator/HadoopUSC">View On GitHub</a></li>
        </ul>

        <p class="header">This project is maintained by 
           <a class="header name" href="http://anrg.usc.edu/~maheswaran">Maheswaran <br/> Sathiamoorthy</a>
           (<a class="header name" href="https://github.com/madiator">@madiator in Github</a>)</p>


      </header>
      <section>
        <h1>Xorbas Hadoop Project Page</h1>

<p>Xorbas is our Hadoop version that implements a new set of regenerating codes called <code>Locally Repairable Codes (LRC)</code>. 
The source code is available <a href="http://github.com/madiator/HadoopUSC/">here</a>.
It is built on top of <a href="http://github.com/facebook/hadoop-20">Facebook's Hadoop system running HDFS-RAID</a>, and thus can support Reed Solomon and XOR codes in addition to LRCs. </p>

<p><img src="http://photos.smahesh.com/photos/i-kq522gx/0/M/i-kq522gx-M.png" alt="Xorbas"></p>

<h2>Paper Title</h2>

<blockquote>
<p>XORing Elephants: Novel Erasure Codes for Big Data</p>
</blockquote>

<h2>Authors:</h2>

<ul>
<li>
<a href="http://anrg.usc.edu/%7Emaheswaran">Maheswaran Sathiamoorthy</a>, University of Southern California</li>
<li>
<a href="http://www-scf.usc.edu/%7Easteris/">Megasthenis Asteris</a>, University of Texas at Austin</li>
<li>
<a href="http://www-scf.usc.edu/%7Epapailio/home.html">Dimitris Papailiopoulos</a>, University of Texas at Austin</li>
<li>
<a href="http://users.ece.utexas.edu/%7Edimakis/">Alexandros G. Dimakis</a>, University of Texas at Austin</li>
<li>Ramkumar Vadali, Facebook</li>
<li>Scott Chen, Facebook</li>
<li>Dhruba Borthakur, Facebook</li>
</ul><h2>Abstract:</h2>

<p>Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability.</p>

<p>This paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance.</p>

<p>We implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2x on the repair disk I/O and repair network traffic. The disadvantage of the new coding scheme is that it requires 14% more storage compared to Reed-Solomon codes, an overhead shown to be information theoretically optimal to obtain locality. Because the new codes repair failures faster, this provides higher reliability, which is orders of magnitude higher compared to replication.</p>

<h2>BibTex</h2>

<pre><code>@article{XorbasVLDB,
    title={XORing Elephants: Novel Erasure Codes for Big Data},
    author={Sathiamoorthy, M. and Asteris, M. and Papailiopoulos, D. and Dimakis, A. G. and Vadali, R. and Chen, S. and Borthakur, D.},
    journal={Proceedings of the VLDB Endowment (to appear)},
    year={2013},
    publisher={VLDB Endowment}
}
</code></pre>


<h3>
<a href="http://anrg.usc.edu/%7Emaheswaran/Xorbas.pdf">Download the Paper</a>.</h3>

<h3>
<a href="http://arxiv.org/abs/1301.3791">Tech Report (arXiv)</a>.</h3>

<h3>Erasure Codes for Large Scale Distributed Storage by Prof Alex Dimakis</h3>
<iframe width="420" height="315" src="http://www.youtube.com/embed/TPZyW_CnXGQ" frameborder="0" allowfullscreen></iframe>

<h4>Note:</h4>
<ol>
<li><h4>The paper is set to appear in 39th International Conference on Very Large Data Bases (VLDB) 2013, Italy.</h4></li>
<li><h4>Part of this work was completed when Megasthenis Asteris, Dimitris Papailiopoulos and Alexandros G. Dimakis were with the University of Southern California </h4></li>
<li><h4>Supported by NSF Award CCF-1055099 and research gifts by Google and Microsoft Research.</h4></li>
</ol>
      </section>
      <footer>
        <p><small>Hosted on <a href="https://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
		          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-7617395-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
